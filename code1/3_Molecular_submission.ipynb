{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "executive-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import re\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
    "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n",
    "    IAAAdditiveGaussianNoise, Transpose, Blur\n",
    "    )\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import timm\n",
    "import Levenshtein\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compact-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './data/'\n",
    "TEST_DIR = PATH + 'test'\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "class CFG : \n",
    "    debug = False\n",
    "    max_len = 275\n",
    "    print_freq = 1000\n",
    "    num_workers = 0\n",
    "    model_name = 'resnet34'\n",
    "    size = 224\n",
    "    scheduler='CosineAnnealingLR'\n",
    "    epochs = 1\n",
    "    T_max = 4 \n",
    "    encoder_lr = 1e-4\n",
    "    decoder_lr = 4e-4\n",
    "    min_lr = 1e-6\n",
    "    batch_size = 16\n",
    "    weight_decay = 1e-6\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 5\n",
    "    attention_dim = 256\n",
    "    embed_dim = 256\n",
    "    decoder_dim = 512\n",
    "    dropout = 0.5\n",
    "    seed = 0\n",
    "    n_fold = 5\n",
    "    trn_fold = [0]\n",
    "    train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "careful-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 1\n",
    "    train = train.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "weighted-picture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0 cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() : \n",
    "    DEVICE = torch.device('cuda')\n",
    "else : \n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print(torch.__version__, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aquatic-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of Memory 해결 법\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "packed-institute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.stoi: {'(': 0, ')': 1, '+': 2, ',': 3, '-': 4, '/b': 5, '/c': 6, '/h': 7, '/i': 8, '/m': 9, '/s': 10, '/t': 11, '0': 12, '1': 13, '10': 14, '100': 15, '101': 16, '102': 17, '103': 18, '104': 19, '105': 20, '106': 21, '107': 22, '108': 23, '109': 24, '11': 25, '110': 26, '111': 27, '112': 28, '113': 29, '114': 30, '115': 31, '116': 32, '117': 33, '118': 34, '119': 35, '12': 36, '120': 37, '121': 38, '122': 39, '123': 40, '124': 41, '125': 42, '126': 43, '127': 44, '128': 45, '129': 46, '13': 47, '130': 48, '131': 49, '132': 50, '133': 51, '134': 52, '135': 53, '136': 54, '137': 55, '138': 56, '139': 57, '14': 58, '140': 59, '141': 60, '142': 61, '143': 62, '144': 63, '145': 64, '146': 65, '147': 66, '148': 67, '149': 68, '15': 69, '150': 70, '151': 71, '152': 72, '153': 73, '154': 74, '155': 75, '156': 76, '157': 77, '158': 78, '159': 79, '16': 80, '161': 81, '163': 82, '165': 83, '167': 84, '17': 85, '18': 86, '19': 87, '2': 88, '20': 89, '21': 90, '22': 91, '23': 92, '24': 93, '25': 94, '26': 95, '27': 96, '28': 97, '29': 98, '3': 99, '30': 100, '31': 101, '32': 102, '33': 103, '34': 104, '35': 105, '36': 106, '37': 107, '38': 108, '39': 109, '4': 110, '40': 111, '41': 112, '42': 113, '43': 114, '44': 115, '45': 116, '46': 117, '47': 118, '48': 119, '49': 120, '5': 121, '50': 122, '51': 123, '52': 124, '53': 125, '54': 126, '55': 127, '56': 128, '57': 129, '58': 130, '59': 131, '6': 132, '60': 133, '61': 134, '62': 135, '63': 136, '64': 137, '65': 138, '66': 139, '67': 140, '68': 141, '69': 142, '7': 143, '70': 144, '71': 145, '72': 146, '73': 147, '74': 148, '75': 149, '76': 150, '77': 151, '78': 152, '79': 153, '8': 154, '80': 155, '81': 156, '82': 157, '83': 158, '84': 159, '85': 160, '86': 161, '87': 162, '88': 163, '89': 164, '9': 165, '90': 166, '91': 167, '92': 168, '93': 169, '94': 170, '95': 171, '96': 172, '97': 173, '98': 174, '99': 175, 'B': 176, 'Br': 177, 'C': 178, 'Cl': 179, 'D': 180, 'F': 181, 'H': 182, 'I': 183, 'N': 184, 'O': 185, 'P': 186, 'S': 187, 'Si': 188, 'T': 189, '<sos>': 190, '<eos>': 191, '<pad>': 192}\n"
     ]
    }
   ],
   "source": [
    "# Code From https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter\n",
    "\n",
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        vocab = set()\n",
    "        for text in texts:\n",
    "            vocab.update(text.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        vocab.append('<sos>')\n",
    "        vocab.append('<eos>')\n",
    "        vocab.append('<pad>')\n",
    "        for i, s in enumerate(vocab):\n",
    "            self.stoi[s] = i\n",
    "        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n",
    "        \n",
    "    def text_to_sequence(self, text):\n",
    "        sequence = []\n",
    "        sequence.append(self.stoi['<sos>'])\n",
    "        for s in text.split(' '):\n",
    "            sequence.append(self.stoi[s])\n",
    "        sequence.append(self.stoi['<eos>'])\n",
    "        return sequence\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = self.text_to_sequence(text)\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def sequence_to_text(self, sequence):\n",
    "        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = self.sequence_to_text(sequence)\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "    def predict_caption(self, sequence):\n",
    "        caption = ''\n",
    "        for i in sequence:\n",
    "            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n",
    "                break\n",
    "            caption += self.itos[i]\n",
    "        return caption\n",
    "    \n",
    "    def predict_captions(self, sequences):\n",
    "        captions = []\n",
    "        for sequence in sequences:\n",
    "            caption = self.predict_caption(sequence)\n",
    "            captions.append(caption)\n",
    "        return captions\n",
    "\n",
    "tokenizer = torch.load('tokenizer2.pth')\n",
    "print(f\"tokenizer.stoi: {tokenizer.stoi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "challenging-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset) : \n",
    "    def __init__(self, df, transform = None) : \n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self) : \n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx) : \n",
    "        path = self.df.path.iloc[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        \n",
    "        augmented = self.transform(image = image)\n",
    "        image = augmented['image']\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "arctic-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transform = Compose([\n",
    "    Resize(CFG.size, CFG.size),\n",
    "    Normalize(\n",
    "        mean = [0.485, 0.456, 0.406],\n",
    "        std = [0.229, 0.224, 0.225],\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "identical-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) : \n",
    "    def __init__(self, model_name = 'resnet34', pretrained = False) :\n",
    "        super().__init__()\n",
    "        self.cnn = timm.create_model(model_name, pretrained = pretrained)\n",
    "        self.n_features = self.cnn.fc.in_features\n",
    "        self.cnn.global_pool = nn.Identity()\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        bs = x.size(0)\n",
    "        features = self.cnn(x)\n",
    "        features = features.permute(0, 2, 3, 1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "floppy-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module) : \n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim) : \n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self, encoder_out, decoder_hidden) : \n",
    "        att1 = self.encoder_att(encoder_out)\n",
    "        att2 = self.decoder_att(decoder_hidden)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n",
    "        alpha = self.softmax(att)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim = 1)\n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "elect-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module) : \n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim = 512, dropout = 0.5) : \n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(p = self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias = True)\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self) : \n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def load_pretrained_embeddings(self, embeddings) : \n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "        \n",
    "    def fine_tune_embeddings(self, fine_tune = True) : \n",
    "        for p in self.embedding.parameters() : \n",
    "            p.requires_grad = fine_tune\n",
    "            \n",
    "    def init_hidden_state(self, encoder_out) : \n",
    "        mean_encoder_out = encoder_out.mean(dim = 1)\n",
    "        h = self.init_h(mean_encoder_out)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths) : \n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        \n",
    "        caption_length, sort_ind = caption_lengths.squeeze(1).sort(dim = 0, descending = True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        embeddings = self.embedding(encoded_captions)\n",
    "        \n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "        \n",
    "        decode_lengths = (caption_length - 1).tolist()\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n",
    "        \n",
    "        for t in range(max(decode_lengths)) : \n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim = 1),\n",
    "                                   (h[:batch_size_t], c[:batch_size_t]))\n",
    "            preds = self.fc(self.dropout(h))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "            \n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
    "    \n",
    "    def predict(self, encoder_out, decode_lengths, tokenizer) : \n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        \n",
    "        start_tockens = torch.ones(batch_size, dtype = torch.long).to(self.device) * tokenizer.stoi[\"<sos>\"]\n",
    "        embeddings = self.embedding(start_tockens)\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).to(self.device)\n",
    "        \n",
    "        for t in range(decode_lengths) : \n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n",
    "            gate = self.sigmoid(self.f_beta(h))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(torch.cat([embeddings, attention_weighted_encoding], dim = 1),\n",
    "                                   (h, c))\n",
    "            preds = self.fc(self.dropout(h))\n",
    "            predictions[:, t, :] = preds\n",
    "            \n",
    "            if np.argmax(preds.detach().cpu().numpy() == tokenizer.stoi[\"<eos>\"]) : \n",
    "                break\n",
    "                \n",
    "            embeddings = self.embedding(torch.argmax(preds, -1))\n",
    "        return predictions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "laden-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred) : \n",
    "    scores = []\n",
    "    \n",
    "    for true, pred in zip(y_true, y_pred) : \n",
    "        score = Levenshtein.distance(true, pred)\n",
    "        scores.append(score)\n",
    "        \n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vietnamese-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "julian-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed = 0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "entire-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object) : \n",
    "    def __init__(self) : \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self) : \n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n = 1) : \n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "logical-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dietary-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bms_collate(batch):\n",
    "    imgs, labels, label_lengths = [], [], []\n",
    "    for data_point in batch:\n",
    "        imgs.append(data_point[0])\n",
    "        labels.append(data_point[1])\n",
    "        label_lengths.append(data_point[2])\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=tokenizer.stoi[\"<pad>\"])\n",
    "    return torch.stack(imgs), labels, torch.stack(label_lengths).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "experienced-genius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.shape: (1616107, 2)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('./data/sample_submission.csv')\n",
    "print(f'test.shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "consolidated-radiation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000d2a601c</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f7fc849</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000037687605</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00004b6d55b6</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00004df0fe53</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id              InChI\n",
       "0  00000d2a601c  InChI=1S/H2O/h1H2\n",
       "1  00001f7fc849  InChI=1S/H2O/h1H2\n",
       "2  000037687605  InChI=1S/H2O/h1H2\n",
       "3  00004b6d55b6  InChI=1S/H2O/h1H2\n",
       "4  00004df0fe53  InChI=1S/H2O/h1H2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "instant-setup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000d2a601c</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f7fc849</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000037687605</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00004b6d55b6</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00004df0fe53</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id              InChI\n",
       "0  00000d2a601c  InChI=1S/H2O/h1H2\n",
       "1  00001f7fc849  InChI=1S/H2O/h1H2\n",
       "2  000037687605  InChI=1S/H2O/h1H2\n",
       "3  00004b6d55b6  InChI=1S/H2O/h1H2\n",
       "4  00004df0fe53  InChI=1S/H2O/h1H2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_test =test.copy()\n",
    "tmp_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "organized-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_path(img_name) : \n",
    "    return f\"{TEST_DIR}/{img_name[0]}/{img_name[1]}/{img_name[2]}/{img_name}.png\"\n",
    "\n",
    "tmp_test['path'] = tmp_test['image_id'].apply(get_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "peripheral-student",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000d2a601c</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "      <td>./data/test/0/0/0/00000d2a601c.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f7fc849</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "      <td>./data/test/0/0/0/00001f7fc849.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000037687605</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "      <td>./data/test/0/0/0/000037687605.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00004b6d55b6</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "      <td>./data/test/0/0/0/00004b6d55b6.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00004df0fe53</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "      <td>./data/test/0/0/0/00004df0fe53.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id              InChI                                path\n",
       "0  00000d2a601c  InChI=1S/H2O/h1H2  ./data/test/0/0/0/00000d2a601c.png\n",
       "1  00001f7fc849  InChI=1S/H2O/h1H2  ./data/test/0/0/0/00001f7fc849.png\n",
       "2  000037687605  InChI=1S/H2O/h1H2  ./data/test/0/0/0/000037687605.png\n",
       "3  00004b6d55b6  InChI=1S/H2O/h1H2  ./data/test/0/0/0/00004b6d55b6.png\n",
       "4  00004df0fe53  InChI=1S/H2O/h1H2  ./data/test/0/0/0/00004df0fe53.png"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adult-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device) : \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    text_preds = []\n",
    "    start = end = time.time()\n",
    "    \n",
    "    for step, (images) in enumerate(valid_loader) : \n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        with torch.no_grad() : \n",
    "            features = encoder(images)\n",
    "            predictions = decoder.predict(features, CFG.max_len, tokenizer)\n",
    "        \n",
    "        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n",
    "        _text_preds = tokenizer.predict_captions(predicted_sequence)\n",
    "        text_preds.append(_text_preds)\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1) :\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  .format(\n",
    "                   step, len(valid_loader), batch_time=batch_time,\n",
    "                   data_time=data_time,\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   ))\n",
    "            \n",
    "    text_preds = np.concatenate(text_preds)\n",
    "    return text_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "widespread-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn() : \n",
    "    test_dataset = TestDataset(tmp_test, Transform)\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size = CFG.batch_size,\n",
    "                             shuffle = False,\n",
    "                             num_workers = CFG.num_workers,\n",
    "                             pin_memory = True,\n",
    "                             drop_last = False)\n",
    "    \n",
    "    encoder = Encoder(CFG.model_name, pretrained = True)\n",
    "    encoder.to(DEVICE)\n",
    "    encoder_optimizer = Adam(encoder.parameters(), lr = CFG.encoder_lr,\n",
    "                             weight_decay = CFG.weight_decay, amsgrad = False)\n",
    "    \n",
    "    decoder = DecoderWithAttention(attention_dim = CFG.attention_dim,\n",
    "                                  embed_dim = CFG.embed_dim,\n",
    "                                  decoder_dim = CFG.decoder_dim,\n",
    "                                  vocab_size = len(tokenizer),\n",
    "                                  dropout = CFG.dropout,\n",
    "                                  device = DEVICE)\n",
    "    decoder.to(DEVICE)\n",
    "    decoder_optimizer = Adam(decoder.parameters(), lr = CFG.decoder_lr,\n",
    "                            weight_decay = CFG.weight_decay, amsgrad = False)\n",
    "    \n",
    "    check_point = torch.load('./resnet34_fold0_best.pth')\n",
    "    \n",
    "    encoder.load_state_dict(check_point['encoder'])\n",
    "    encoder_optimizer.load_state_dict(check_point['encoder_optimizer'])\n",
    "    decoder.load_state_dict(check_point['decoder'])\n",
    "    decoder_optimizer.load_state_dict(check_point['decoder_optimizer'])\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.stoi[\"<pad>\"])\n",
    "    \n",
    "    text_preds = valid_fn(test_loader, encoder, decoder, tokenizer, criterion, DEVICE)\n",
    "    text_preds = [f\"InChI=1S/{text}\" for text in text_preds]\n",
    "    \n",
    "    return text_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "behind-demand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/101007] Data 0.110 (0.110) Elapsed 0m 3s (remain 5256m 43s) \n",
      "EVAL: [1000/101007] Data 0.083 (0.086) Elapsed 5m 39s (remain 564m 52s) \n",
      "EVAL: [2000/101007] Data 0.086 (0.091) Elapsed 11m 54s (remain 589m 26s) \n",
      "EVAL: [3000/101007] Data 0.094 (0.091) Elapsed 18m 1s (remain 588m 23s) \n",
      "EVAL: [4000/101007] Data 0.092 (0.092) Elapsed 24m 12s (remain 586m 47s) \n",
      "EVAL: [5000/101007] Data 0.085 (0.092) Elapsed 30m 24s (remain 583m 47s) \n",
      "EVAL: [6000/101007] Data 0.081 (0.093) Elapsed 36m 47s (remain 582m 23s) \n",
      "EVAL: [7000/101007] Data 0.084 (0.094) Elapsed 43m 22s (remain 582m 28s) \n",
      "EVAL: [8000/101007] Data 0.100 (0.094) Elapsed 49m 55s (remain 580m 20s) \n",
      "EVAL: [9000/101007] Data 0.106 (0.094) Elapsed 56m 12s (remain 574m 33s) \n",
      "EVAL: [10000/101007] Data 0.097 (0.094) Elapsed 62m 20s (remain 567m 21s) \n",
      "EVAL: [11000/101007] Data 0.087 (0.094) Elapsed 68m 23s (remain 559m 36s) \n",
      "EVAL: [12000/101007] Data 0.092 (0.093) Elapsed 74m 26s (remain 552m 5s) \n",
      "EVAL: [13000/101007] Data 0.103 (0.093) Elapsed 80m 41s (remain 546m 10s) \n",
      "EVAL: [14000/101007] Data 0.092 (0.093) Elapsed 86m 56s (remain 540m 13s) \n",
      "EVAL: [15000/101007] Data 0.100 (0.093) Elapsed 93m 10s (remain 534m 11s) \n",
      "EVAL: [16000/101007] Data 0.104 (0.093) Elapsed 99m 16s (remain 527m 24s) \n",
      "EVAL: [17000/101007] Data 0.084 (0.093) Elapsed 105m 18s (remain 520m 19s) \n",
      "EVAL: [18000/101007] Data 0.096 (0.093) Elapsed 111m 28s (remain 514m 2s) \n",
      "EVAL: [19000/101007] Data 0.092 (0.093) Elapsed 117m 40s (remain 507m 54s) \n",
      "EVAL: [20000/101007] Data 0.088 (0.093) Elapsed 123m 51s (remain 501m 38s) \n",
      "EVAL: [21000/101007] Data 0.092 (0.093) Elapsed 130m 5s (remain 495m 35s) \n",
      "EVAL: [22000/101007] Data 0.121 (0.094) Elapsed 136m 35s (remain 490m 29s) \n",
      "EVAL: [23000/101007] Data 0.082 (0.094) Elapsed 142m 53s (remain 484m 35s) \n",
      "EVAL: [24000/101007] Data 0.102 (0.094) Elapsed 149m 56s (remain 481m 6s) \n",
      "EVAL: [25000/101007] Data 0.156 (0.095) Elapsed 156m 53s (remain 476m 57s) \n",
      "EVAL: [26000/101007] Data 0.102 (0.095) Elapsed 164m 2s (remain 473m 14s) \n",
      "EVAL: [27000/101007] Data 0.124 (0.096) Elapsed 171m 30s (remain 470m 6s) \n",
      "EVAL: [28000/101007] Data 0.097 (0.096) Elapsed 177m 47s (remain 463m 33s) \n",
      "EVAL: [29000/101007] Data 0.087 (0.096) Elapsed 184m 5s (remain 457m 4s) \n",
      "EVAL: [30000/101007] Data 0.114 (0.096) Elapsed 190m 10s (remain 450m 5s) \n",
      "EVAL: [31000/101007] Data 0.087 (0.095) Elapsed 196m 13s (remain 443m 7s) \n",
      "EVAL: [32000/101007] Data 0.085 (0.096) Elapsed 202m 38s (remain 436m 59s) \n",
      "EVAL: [33000/101007] Data 0.087 (0.096) Elapsed 208m 57s (remain 430m 36s) \n",
      "EVAL: [34000/101007] Data 0.103 (0.096) Elapsed 215m 20s (remain 424m 23s) \n",
      "EVAL: [35000/101007] Data 0.093 (0.096) Elapsed 221m 35s (remain 417m 52s) \n",
      "EVAL: [36000/101007] Data 0.101 (0.096) Elapsed 228m 8s (remain 411m 57s) \n",
      "EVAL: [37000/101007] Data 0.096 (0.096) Elapsed 234m 26s (remain 405m 32s) \n",
      "EVAL: [38000/101007] Data 0.101 (0.096) Elapsed 240m 44s (remain 399m 9s) \n",
      "EVAL: [39000/101007] Data 0.091 (0.096) Elapsed 246m 58s (remain 392m 38s) \n",
      "EVAL: [40000/101007] Data 0.094 (0.096) Elapsed 253m 11s (remain 386m 8s) \n",
      "EVAL: [41000/101007] Data 0.086 (0.096) Elapsed 259m 25s (remain 379m 40s) \n",
      "EVAL: [42000/101007] Data 0.084 (0.096) Elapsed 265m 39s (remain 373m 12s) \n",
      "EVAL: [43000/101007] Data 0.128 (0.096) Elapsed 271m 51s (remain 366m 42s) \n",
      "EVAL: [44000/101007] Data 0.092 (0.096) Elapsed 278m 13s (remain 360m 27s) \n",
      "EVAL: [45000/101007] Data 0.170 (0.096) Elapsed 284m 39s (remain 354m 15s) \n",
      "EVAL: [46000/101007] Data 0.145 (0.096) Elapsed 291m 10s (remain 348m 10s) \n",
      "EVAL: [47000/101007] Data 0.101 (0.096) Elapsed 298m 44s (remain 343m 16s) \n",
      "EVAL: [48000/101007] Data 0.108 (0.096) Elapsed 305m 10s (remain 336m 59s) \n",
      "EVAL: [49000/101007] Data 0.104 (0.096) Elapsed 312m 51s (remain 332m 2s) \n",
      "EVAL: [50000/101007] Data 0.117 (0.097) Elapsed 320m 36s (remain 327m 3s) \n",
      "EVAL: [51000/101007] Data 0.102 (0.097) Elapsed 328m 17s (remain 321m 53s) \n",
      "EVAL: [52000/101007] Data 0.115 (0.097) Elapsed 336m 4s (remain 316m 43s) \n",
      "EVAL: [53000/101007] Data 0.096 (0.097) Elapsed 343m 6s (remain 310m 46s) \n",
      "EVAL: [54000/101007] Data 0.131 (0.097) Elapsed 349m 22s (remain 304m 6s) \n",
      "EVAL: [55000/101007] Data 0.083 (0.097) Elapsed 355m 20s (remain 297m 13s) \n",
      "EVAL: [56000/101007] Data 0.113 (0.097) Elapsed 361m 31s (remain 290m 32s) \n",
      "EVAL: [57000/101007] Data 0.091 (0.097) Elapsed 367m 52s (remain 284m 0s) \n",
      "EVAL: [58000/101007] Data 0.124 (0.097) Elapsed 374m 25s (remain 277m 37s) \n",
      "EVAL: [59000/101007] Data 0.095 (0.098) Elapsed 381m 28s (remain 271m 35s) \n",
      "EVAL: [60000/101007] Data 0.090 (0.098) Elapsed 388m 15s (remain 265m 20s) \n",
      "EVAL: [61000/101007] Data 0.090 (0.098) Elapsed 394m 57s (remain 259m 1s) \n",
      "EVAL: [62000/101007] Data 0.089 (0.098) Elapsed 401m 34s (remain 252m 38s) \n",
      "EVAL: [63000/101007] Data 0.097 (0.098) Elapsed 408m 17s (remain 246m 18s) \n",
      "EVAL: [64000/101007] Data 0.107 (0.098) Elapsed 414m 58s (remain 239m 56s) \n",
      "EVAL: [65000/101007] Data 0.099 (0.098) Elapsed 421m 47s (remain 233m 38s) \n",
      "EVAL: [66000/101007] Data 0.114 (0.098) Elapsed 428m 36s (remain 227m 19s) \n",
      "EVAL: [67000/101007] Data 0.106 (0.098) Elapsed 435m 17s (remain 220m 55s) \n",
      "EVAL: [68000/101007] Data 0.106 (0.098) Elapsed 441m 56s (remain 214m 30s) \n",
      "EVAL: [69000/101007] Data 0.113 (0.098) Elapsed 448m 26s (remain 208m 0s) \n",
      "EVAL: [70000/101007] Data 0.092 (0.098) Elapsed 454m 40s (remain 201m 23s) \n",
      "EVAL: [71000/101007] Data 0.114 (0.098) Elapsed 460m 40s (remain 194m 41s) \n",
      "EVAL: [72000/101007] Data 0.100 (0.098) Elapsed 467m 52s (remain 188m 29s) \n",
      "EVAL: [73000/101007] Data 0.105 (0.098) Elapsed 474m 57s (remain 182m 12s) \n",
      "EVAL: [74000/101007] Data 0.102 (0.098) Elapsed 482m 4s (remain 175m 55s) \n",
      "EVAL: [75000/101007] Data 0.096 (0.098) Elapsed 489m 12s (remain 169m 37s) \n",
      "EVAL: [76000/101007] Data 0.094 (0.099) Elapsed 496m 26s (remain 163m 20s) \n",
      "EVAL: [77000/101007] Data 0.079 (0.098) Elapsed 502m 36s (remain 156m 41s) \n",
      "EVAL: [78000/101007] Data 0.090 (0.098) Elapsed 508m 21s (remain 149m 56s) \n",
      "EVAL: [79000/101007] Data 0.081 (0.098) Elapsed 513m 52s (remain 143m 8s) \n",
      "EVAL: [80000/101007] Data 0.093 (0.098) Elapsed 519m 29s (remain 136m 24s) \n",
      "EVAL: [81000/101007] Data 0.081 (0.098) Elapsed 525m 11s (remain 129m 42s) \n",
      "EVAL: [82000/101007] Data 0.088 (0.098) Elapsed 530m 51s (remain 123m 2s) \n",
      "EVAL: [83000/101007] Data 0.137 (0.097) Elapsed 537m 4s (remain 116m 30s) \n",
      "EVAL: [84000/101007] Data 0.112 (0.098) Elapsed 552m 13s (remain 111m 47s) \n",
      "EVAL: [85000/101007] Data 0.108 (0.098) Elapsed 567m 25s (remain 106m 50s) \n",
      "EVAL: [86000/101007] Data 0.116 (0.098) Elapsed 583m 11s (remain 101m 45s) \n",
      "EVAL: [87000/101007] Data 0.115 (0.099) Elapsed 599m 9s (remain 96m 27s) \n",
      "EVAL: [88000/101007] Data 0.133 (0.099) Elapsed 615m 2s (remain 90m 54s) \n",
      "EVAL: [89000/101007] Data 0.082 (0.099) Elapsed 623m 2s (remain 84m 2s) \n",
      "EVAL: [90000/101007] Data 0.084 (0.099) Elapsed 628m 59s (remain 76m 55s) \n",
      "EVAL: [91000/101007] Data 0.079 (0.099) Elapsed 634m 46s (remain 69m 47s) \n",
      "EVAL: [92000/101007] Data 0.085 (0.099) Elapsed 640m 32s (remain 62m 42s) \n",
      "EVAL: [93000/101007] Data 0.098 (0.099) Elapsed 648m 29s (remain 55m 49s) \n",
      "EVAL: [94000/101007] Data 0.081 (0.099) Elapsed 655m 2s (remain 48m 49s) \n",
      "EVAL: [95000/101007] Data 0.078 (0.099) Elapsed 660m 56s (remain 41m 47s) \n",
      "EVAL: [96000/101007] Data 0.079 (0.098) Elapsed 666m 28s (remain 34m 45s) \n",
      "EVAL: [97000/101007] Data 0.079 (0.098) Elapsed 671m 58s (remain 27m 45s) \n",
      "EVAL: [98000/101007] Data 0.082 (0.098) Elapsed 678m 10s (remain 20m 48s) \n",
      "EVAL: [99000/101007] Data 0.089 (0.098) Elapsed 683m 49s (remain 13m 51s) \n",
      "EVAL: [100000/101007] Data 0.077 (0.098) Elapsed 689m 20s (remain 6m 56s) \n",
      "EVAL: [101000/101007] Data 0.078 (0.098) Elapsed 695m 15s (remain 0m 2s) \n",
      "EVAL: [101006/101007] Data 0.058 (0.098) Elapsed 695m 16s (remain 0m 0s) \n"
     ]
    }
   ],
   "source": [
    "text_preds = test_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "marine-morgan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "281000\n",
      "282000\n",
      "283000\n",
      "284000\n",
      "285000\n",
      "286000\n",
      "287000\n",
      "288000\n",
      "289000\n",
      "290000\n",
      "291000\n",
      "292000\n",
      "293000\n",
      "294000\n",
      "295000\n",
      "296000\n",
      "297000\n",
      "298000\n",
      "299000\n",
      "300000\n",
      "301000\n",
      "302000\n",
      "303000\n",
      "304000\n",
      "305000\n",
      "306000\n",
      "307000\n",
      "308000\n",
      "309000\n",
      "310000\n",
      "311000\n",
      "312000\n",
      "313000\n",
      "314000\n",
      "315000\n",
      "316000\n",
      "317000\n",
      "318000\n",
      "319000\n",
      "320000\n",
      "321000\n",
      "322000\n",
      "323000\n",
      "324000\n",
      "325000\n",
      "326000\n",
      "327000\n",
      "328000\n",
      "329000\n",
      "330000\n",
      "331000\n",
      "332000\n",
      "333000\n",
      "334000\n",
      "335000\n",
      "336000\n",
      "337000\n",
      "338000\n",
      "339000\n",
      "340000\n",
      "341000\n",
      "342000\n",
      "343000\n",
      "344000\n",
      "345000\n",
      "346000\n",
      "347000\n",
      "348000\n",
      "349000\n",
      "350000\n",
      "351000\n",
      "352000\n",
      "353000\n",
      "354000\n",
      "355000\n",
      "356000\n",
      "357000\n",
      "358000\n",
      "359000\n",
      "360000\n",
      "361000\n",
      "362000\n",
      "363000\n",
      "364000\n",
      "365000\n",
      "366000\n",
      "367000\n",
      "368000\n",
      "369000\n",
      "370000\n",
      "371000\n",
      "372000\n",
      "373000\n",
      "374000\n",
      "375000\n",
      "376000\n",
      "377000\n",
      "378000\n",
      "379000\n",
      "380000\n",
      "381000\n",
      "382000\n",
      "383000\n",
      "384000\n",
      "385000\n",
      "386000\n",
      "387000\n",
      "388000\n",
      "389000\n",
      "390000\n",
      "391000\n",
      "392000\n",
      "393000\n",
      "394000\n",
      "395000\n",
      "396000\n",
      "397000\n",
      "398000\n",
      "399000\n",
      "400000\n",
      "401000\n",
      "402000\n",
      "403000\n",
      "404000\n",
      "405000\n",
      "406000\n",
      "407000\n",
      "408000\n",
      "409000\n",
      "410000\n",
      "411000\n",
      "412000\n",
      "413000\n",
      "414000\n",
      "415000\n",
      "416000\n",
      "417000\n",
      "418000\n",
      "419000\n",
      "420000\n",
      "421000\n",
      "422000\n",
      "423000\n",
      "424000\n",
      "425000\n",
      "426000\n",
      "427000\n",
      "428000\n",
      "429000\n",
      "430000\n",
      "431000\n",
      "432000\n",
      "433000\n",
      "434000\n",
      "435000\n",
      "436000\n",
      "437000\n",
      "438000\n",
      "439000\n",
      "440000\n",
      "441000\n",
      "442000\n",
      "443000\n",
      "444000\n",
      "445000\n",
      "446000\n",
      "447000\n",
      "448000\n",
      "449000\n",
      "450000\n",
      "451000\n",
      "452000\n",
      "453000\n",
      "454000\n",
      "455000\n",
      "456000\n",
      "457000\n",
      "458000\n",
      "459000\n",
      "460000\n",
      "461000\n",
      "462000\n",
      "463000\n",
      "464000\n",
      "465000\n",
      "466000\n",
      "467000\n",
      "468000\n",
      "469000\n",
      "470000\n",
      "471000\n",
      "472000\n",
      "473000\n",
      "474000\n",
      "475000\n",
      "476000\n",
      "477000\n",
      "478000\n",
      "479000\n",
      "480000\n",
      "481000\n",
      "482000\n",
      "483000\n",
      "484000\n",
      "485000\n",
      "486000\n",
      "487000\n",
      "488000\n",
      "489000\n",
      "490000\n",
      "491000\n",
      "492000\n",
      "493000\n",
      "494000\n",
      "495000\n",
      "496000\n",
      "497000\n",
      "498000\n",
      "499000\n",
      "500000\n",
      "501000\n",
      "502000\n",
      "503000\n",
      "504000\n",
      "505000\n",
      "506000\n",
      "507000\n",
      "508000\n",
      "509000\n",
      "510000\n",
      "511000\n",
      "512000\n",
      "513000\n",
      "514000\n",
      "515000\n",
      "516000\n",
      "517000\n",
      "518000\n",
      "519000\n",
      "520000\n",
      "521000\n",
      "522000\n",
      "523000\n",
      "524000\n",
      "525000\n",
      "526000\n",
      "527000\n",
      "528000\n",
      "529000\n",
      "530000\n",
      "531000\n",
      "532000\n",
      "533000\n",
      "534000\n",
      "535000\n",
      "536000\n",
      "537000\n",
      "538000\n",
      "539000\n",
      "540000\n",
      "541000\n",
      "542000\n",
      "543000\n",
      "544000\n",
      "545000\n",
      "546000\n",
      "547000\n",
      "548000\n",
      "549000\n",
      "550000\n",
      "551000\n",
      "552000\n",
      "553000\n",
      "554000\n",
      "555000\n",
      "556000\n",
      "557000\n",
      "558000\n",
      "559000\n",
      "560000\n",
      "561000\n",
      "562000\n",
      "563000\n",
      "564000\n",
      "565000\n",
      "566000\n",
      "567000\n",
      "568000\n",
      "569000\n",
      "570000\n",
      "571000\n",
      "572000\n",
      "573000\n",
      "574000\n",
      "575000\n",
      "576000\n",
      "577000\n",
      "578000\n",
      "579000\n",
      "580000\n",
      "581000\n",
      "582000\n",
      "583000\n",
      "584000\n",
      "585000\n",
      "586000\n",
      "587000\n",
      "588000\n",
      "589000\n",
      "590000\n",
      "591000\n",
      "592000\n",
      "593000\n",
      "594000\n",
      "595000\n",
      "596000\n",
      "597000\n",
      "598000\n",
      "599000\n",
      "600000\n",
      "601000\n",
      "602000\n",
      "603000\n",
      "604000\n",
      "605000\n",
      "606000\n",
      "607000\n",
      "608000\n",
      "609000\n",
      "610000\n",
      "611000\n",
      "612000\n",
      "613000\n",
      "614000\n",
      "615000\n",
      "616000\n",
      "617000\n",
      "618000\n",
      "619000\n",
      "620000\n",
      "621000\n",
      "622000\n",
      "623000\n",
      "624000\n",
      "625000\n",
      "626000\n",
      "627000\n",
      "628000\n",
      "629000\n",
      "630000\n",
      "631000\n",
      "632000\n",
      "633000\n",
      "634000\n",
      "635000\n",
      "636000\n",
      "637000\n",
      "638000\n",
      "639000\n",
      "640000\n",
      "641000\n",
      "642000\n",
      "643000\n",
      "644000\n",
      "645000\n",
      "646000\n",
      "647000\n",
      "648000\n",
      "649000\n",
      "650000\n",
      "651000\n",
      "652000\n",
      "653000\n",
      "654000\n",
      "655000\n",
      "656000\n",
      "657000\n",
      "658000\n",
      "659000\n",
      "660000\n",
      "661000\n",
      "662000\n",
      "663000\n",
      "664000\n",
      "665000\n",
      "666000\n",
      "667000\n",
      "668000\n",
      "669000\n",
      "670000\n",
      "671000\n",
      "672000\n",
      "673000\n",
      "674000\n",
      "675000\n",
      "676000\n",
      "677000\n",
      "678000\n",
      "679000\n",
      "680000\n",
      "681000\n",
      "682000\n",
      "683000\n",
      "684000\n",
      "685000\n",
      "686000\n",
      "687000\n",
      "688000\n",
      "689000\n",
      "690000\n",
      "691000\n",
      "692000\n",
      "693000\n",
      "694000\n",
      "695000\n",
      "696000\n",
      "697000\n",
      "698000\n",
      "699000\n",
      "700000\n",
      "701000\n",
      "702000\n",
      "703000\n",
      "704000\n",
      "705000\n",
      "706000\n",
      "707000\n",
      "708000\n",
      "709000\n",
      "710000\n",
      "711000\n",
      "712000\n",
      "713000\n",
      "714000\n",
      "715000\n",
      "716000\n",
      "717000\n",
      "718000\n",
      "719000\n",
      "720000\n",
      "721000\n",
      "722000\n",
      "723000\n",
      "724000\n",
      "725000\n",
      "726000\n",
      "727000\n",
      "728000\n",
      "729000\n",
      "730000\n",
      "731000\n",
      "732000\n",
      "733000\n",
      "734000\n",
      "735000\n",
      "736000\n",
      "737000\n",
      "738000\n",
      "739000\n",
      "740000\n",
      "741000\n",
      "742000\n",
      "743000\n",
      "744000\n",
      "745000\n",
      "746000\n",
      "747000\n",
      "748000\n",
      "749000\n",
      "750000\n",
      "751000\n",
      "752000\n",
      "753000\n",
      "754000\n",
      "755000\n",
      "756000\n",
      "757000\n",
      "758000\n",
      "759000\n",
      "760000\n",
      "761000\n",
      "762000\n",
      "763000\n",
      "764000\n",
      "765000\n",
      "766000\n",
      "767000\n",
      "768000\n",
      "769000\n",
      "770000\n",
      "771000\n",
      "772000\n",
      "773000\n",
      "774000\n",
      "775000\n",
      "776000\n",
      "777000\n",
      "778000\n",
      "779000\n",
      "780000\n",
      "781000\n",
      "782000\n",
      "783000\n",
      "784000\n",
      "785000\n",
      "786000\n",
      "787000\n",
      "788000\n",
      "789000\n",
      "790000\n",
      "791000\n",
      "792000\n",
      "793000\n",
      "794000\n",
      "795000\n",
      "796000\n",
      "797000\n",
      "798000\n",
      "799000\n",
      "800000\n",
      "801000\n",
      "802000\n",
      "803000\n",
      "804000\n",
      "805000\n",
      "806000\n",
      "807000\n",
      "808000\n",
      "809000\n",
      "810000\n",
      "811000\n",
      "812000\n",
      "813000\n",
      "814000\n",
      "815000\n",
      "816000\n",
      "817000\n",
      "818000\n",
      "819000\n",
      "820000\n",
      "821000\n",
      "822000\n",
      "823000\n",
      "824000\n",
      "825000\n",
      "826000\n",
      "827000\n",
      "828000\n",
      "829000\n",
      "830000\n",
      "831000\n",
      "832000\n",
      "833000\n",
      "834000\n",
      "835000\n",
      "836000\n",
      "837000\n",
      "838000\n",
      "839000\n",
      "840000\n",
      "841000\n",
      "842000\n",
      "843000\n",
      "844000\n",
      "845000\n",
      "846000\n",
      "847000\n",
      "848000\n",
      "849000\n",
      "850000\n",
      "851000\n",
      "852000\n",
      "853000\n",
      "854000\n",
      "855000\n",
      "856000\n",
      "857000\n",
      "858000\n",
      "859000\n",
      "860000\n",
      "861000\n",
      "862000\n",
      "863000\n",
      "864000\n",
      "865000\n",
      "866000\n",
      "867000\n",
      "868000\n",
      "869000\n",
      "870000\n",
      "871000\n",
      "872000\n",
      "873000\n",
      "874000\n",
      "875000\n",
      "876000\n",
      "877000\n",
      "878000\n",
      "879000\n",
      "880000\n",
      "881000\n",
      "882000\n",
      "883000\n",
      "884000\n",
      "885000\n",
      "886000\n",
      "887000\n",
      "888000\n",
      "889000\n",
      "890000\n",
      "891000\n",
      "892000\n",
      "893000\n",
      "894000\n",
      "895000\n",
      "896000\n",
      "897000\n",
      "898000\n",
      "899000\n",
      "900000\n",
      "901000\n",
      "902000\n",
      "903000\n",
      "904000\n",
      "905000\n",
      "906000\n",
      "907000\n",
      "908000\n",
      "909000\n",
      "910000\n",
      "911000\n",
      "912000\n",
      "913000\n",
      "914000\n",
      "915000\n",
      "916000\n",
      "917000\n",
      "918000\n",
      "919000\n",
      "920000\n",
      "921000\n",
      "922000\n",
      "923000\n",
      "924000\n",
      "925000\n",
      "926000\n",
      "927000\n",
      "928000\n",
      "929000\n",
      "930000\n",
      "931000\n",
      "932000\n",
      "933000\n",
      "934000\n",
      "935000\n",
      "936000\n",
      "937000\n",
      "938000\n",
      "939000\n",
      "940000\n",
      "941000\n",
      "942000\n",
      "943000\n",
      "944000\n",
      "945000\n",
      "946000\n",
      "947000\n",
      "948000\n",
      "949000\n",
      "950000\n",
      "951000\n",
      "952000\n",
      "953000\n",
      "954000\n",
      "955000\n",
      "956000\n",
      "957000\n",
      "958000\n",
      "959000\n",
      "960000\n",
      "961000\n",
      "962000\n",
      "963000\n",
      "964000\n",
      "965000\n",
      "966000\n",
      "967000\n",
      "968000\n",
      "969000\n",
      "970000\n",
      "971000\n",
      "972000\n",
      "973000\n",
      "974000\n",
      "975000\n",
      "976000\n",
      "977000\n",
      "978000\n",
      "979000\n",
      "980000\n",
      "981000\n",
      "982000\n",
      "983000\n",
      "984000\n",
      "985000\n",
      "986000\n",
      "987000\n",
      "988000\n",
      "989000\n",
      "990000\n",
      "991000\n",
      "992000\n",
      "993000\n",
      "994000\n",
      "995000\n",
      "996000\n",
      "997000\n",
      "998000\n",
      "999000\n",
      "1000000\n",
      "1001000\n",
      "1002000\n",
      "1003000\n",
      "1004000\n",
      "1005000\n",
      "1006000\n",
      "1007000\n",
      "1008000\n",
      "1009000\n",
      "1010000\n",
      "1011000\n",
      "1012000\n",
      "1013000\n",
      "1014000\n",
      "1015000\n",
      "1016000\n",
      "1017000\n",
      "1018000\n",
      "1019000\n",
      "1020000\n",
      "1021000\n",
      "1022000\n",
      "1023000\n",
      "1024000\n",
      "1025000\n",
      "1026000\n",
      "1027000\n",
      "1028000\n",
      "1029000\n",
      "1030000\n",
      "1031000\n",
      "1032000\n",
      "1033000\n",
      "1034000\n",
      "1035000\n",
      "1036000\n",
      "1037000\n",
      "1038000\n",
      "1039000\n",
      "1040000\n",
      "1041000\n",
      "1042000\n",
      "1043000\n",
      "1044000\n",
      "1045000\n",
      "1046000\n",
      "1047000\n",
      "1048000\n",
      "1049000\n",
      "1050000\n",
      "1051000\n",
      "1052000\n",
      "1053000\n",
      "1054000\n",
      "1055000\n",
      "1056000\n",
      "1057000\n",
      "1058000\n",
      "1059000\n",
      "1060000\n",
      "1061000\n",
      "1062000\n",
      "1063000\n",
      "1064000\n",
      "1065000\n",
      "1066000\n",
      "1067000\n",
      "1068000\n",
      "1069000\n",
      "1070000\n",
      "1071000\n",
      "1072000\n",
      "1073000\n",
      "1074000\n",
      "1075000\n",
      "1076000\n",
      "1077000\n",
      "1078000\n",
      "1079000\n",
      "1080000\n",
      "1081000\n",
      "1082000\n",
      "1083000\n",
      "1084000\n",
      "1085000\n",
      "1086000\n",
      "1087000\n",
      "1088000\n",
      "1089000\n",
      "1090000\n",
      "1091000\n",
      "1092000\n",
      "1093000\n",
      "1094000\n",
      "1095000\n",
      "1096000\n",
      "1097000\n",
      "1098000\n",
      "1099000\n",
      "1100000\n",
      "1101000\n",
      "1102000\n",
      "1103000\n",
      "1104000\n",
      "1105000\n",
      "1106000\n",
      "1107000\n",
      "1108000\n",
      "1109000\n",
      "1110000\n",
      "1111000\n",
      "1112000\n",
      "1113000\n",
      "1114000\n",
      "1115000\n",
      "1116000\n",
      "1117000\n",
      "1118000\n",
      "1119000\n",
      "1120000\n",
      "1121000\n",
      "1122000\n",
      "1123000\n",
      "1124000\n",
      "1125000\n",
      "1126000\n",
      "1127000\n",
      "1128000\n",
      "1129000\n",
      "1130000\n",
      "1131000\n",
      "1132000\n",
      "1133000\n",
      "1134000\n",
      "1135000\n",
      "1136000\n",
      "1137000\n",
      "1138000\n",
      "1139000\n",
      "1140000\n",
      "1141000\n",
      "1142000\n",
      "1143000\n",
      "1144000\n",
      "1145000\n",
      "1146000\n",
      "1147000\n",
      "1148000\n",
      "1149000\n",
      "1150000\n",
      "1151000\n",
      "1152000\n",
      "1153000\n",
      "1154000\n",
      "1155000\n",
      "1156000\n",
      "1157000\n",
      "1158000\n",
      "1159000\n",
      "1160000\n",
      "1161000\n",
      "1162000\n",
      "1163000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164000\n",
      "1165000\n",
      "1166000\n",
      "1167000\n",
      "1168000\n",
      "1169000\n",
      "1170000\n",
      "1171000\n",
      "1172000\n",
      "1173000\n",
      "1174000\n",
      "1175000\n",
      "1176000\n",
      "1177000\n",
      "1178000\n",
      "1179000\n",
      "1180000\n",
      "1181000\n",
      "1182000\n",
      "1183000\n",
      "1184000\n",
      "1185000\n",
      "1186000\n",
      "1187000\n",
      "1188000\n",
      "1189000\n",
      "1190000\n",
      "1191000\n",
      "1192000\n",
      "1193000\n",
      "1194000\n",
      "1195000\n",
      "1196000\n",
      "1197000\n",
      "1198000\n",
      "1199000\n",
      "1200000\n",
      "1201000\n",
      "1202000\n",
      "1203000\n",
      "1204000\n",
      "1205000\n",
      "1206000\n",
      "1207000\n",
      "1208000\n",
      "1209000\n",
      "1210000\n",
      "1211000\n",
      "1212000\n",
      "1213000\n",
      "1214000\n",
      "1215000\n",
      "1216000\n",
      "1217000\n",
      "1218000\n",
      "1219000\n",
      "1220000\n",
      "1221000\n",
      "1222000\n",
      "1223000\n",
      "1224000\n",
      "1225000\n",
      "1226000\n",
      "1227000\n",
      "1228000\n",
      "1229000\n",
      "1230000\n",
      "1231000\n",
      "1232000\n",
      "1233000\n",
      "1234000\n",
      "1235000\n",
      "1236000\n",
      "1237000\n",
      "1238000\n",
      "1239000\n",
      "1240000\n",
      "1241000\n",
      "1242000\n",
      "1243000\n",
      "1244000\n",
      "1245000\n",
      "1246000\n",
      "1247000\n",
      "1248000\n",
      "1249000\n",
      "1250000\n",
      "1251000\n",
      "1252000\n",
      "1253000\n",
      "1254000\n",
      "1255000\n",
      "1256000\n",
      "1257000\n",
      "1258000\n",
      "1259000\n",
      "1260000\n",
      "1261000\n",
      "1262000\n",
      "1263000\n",
      "1264000\n",
      "1265000\n",
      "1266000\n",
      "1267000\n",
      "1268000\n",
      "1269000\n",
      "1270000\n",
      "1271000\n",
      "1272000\n",
      "1273000\n",
      "1274000\n",
      "1275000\n",
      "1276000\n",
      "1277000\n",
      "1278000\n",
      "1279000\n",
      "1280000\n",
      "1281000\n",
      "1282000\n",
      "1283000\n",
      "1284000\n",
      "1285000\n",
      "1286000\n",
      "1287000\n",
      "1288000\n",
      "1289000\n",
      "1290000\n",
      "1291000\n",
      "1292000\n",
      "1293000\n",
      "1294000\n",
      "1295000\n",
      "1296000\n",
      "1297000\n",
      "1298000\n",
      "1299000\n",
      "1300000\n",
      "1301000\n",
      "1302000\n",
      "1303000\n",
      "1304000\n",
      "1305000\n",
      "1306000\n",
      "1307000\n",
      "1308000\n",
      "1309000\n",
      "1310000\n",
      "1311000\n",
      "1312000\n",
      "1313000\n",
      "1314000\n",
      "1315000\n",
      "1316000\n",
      "1317000\n",
      "1318000\n",
      "1319000\n",
      "1320000\n",
      "1321000\n",
      "1322000\n",
      "1323000\n",
      "1324000\n",
      "1325000\n",
      "1326000\n",
      "1327000\n",
      "1328000\n",
      "1329000\n",
      "1330000\n",
      "1331000\n",
      "1332000\n",
      "1333000\n",
      "1334000\n",
      "1335000\n",
      "1336000\n",
      "1337000\n",
      "1338000\n",
      "1339000\n",
      "1340000\n",
      "1341000\n",
      "1342000\n",
      "1343000\n",
      "1344000\n",
      "1345000\n",
      "1346000\n",
      "1347000\n",
      "1348000\n",
      "1349000\n",
      "1350000\n",
      "1351000\n",
      "1352000\n",
      "1353000\n",
      "1354000\n",
      "1355000\n",
      "1356000\n",
      "1357000\n",
      "1358000\n",
      "1359000\n",
      "1360000\n",
      "1361000\n",
      "1362000\n",
      "1363000\n",
      "1364000\n",
      "1365000\n",
      "1366000\n",
      "1367000\n",
      "1368000\n",
      "1369000\n",
      "1370000\n",
      "1371000\n",
      "1372000\n",
      "1373000\n",
      "1374000\n",
      "1375000\n",
      "1376000\n",
      "1377000\n",
      "1378000\n",
      "1379000\n",
      "1380000\n",
      "1381000\n",
      "1382000\n",
      "1383000\n",
      "1384000\n",
      "1385000\n",
      "1386000\n",
      "1387000\n",
      "1388000\n",
      "1389000\n",
      "1390000\n",
      "1391000\n",
      "1392000\n",
      "1393000\n",
      "1394000\n",
      "1395000\n",
      "1396000\n",
      "1397000\n",
      "1398000\n",
      "1399000\n",
      "1400000\n",
      "1401000\n",
      "1402000\n",
      "1403000\n",
      "1404000\n",
      "1405000\n",
      "1406000\n",
      "1407000\n",
      "1408000\n",
      "1409000\n",
      "1410000\n",
      "1411000\n",
      "1412000\n",
      "1413000\n",
      "1414000\n",
      "1415000\n",
      "1416000\n",
      "1417000\n",
      "1418000\n",
      "1419000\n",
      "1420000\n",
      "1421000\n",
      "1422000\n",
      "1423000\n",
      "1424000\n",
      "1425000\n",
      "1426000\n",
      "1427000\n",
      "1428000\n",
      "1429000\n",
      "1430000\n",
      "1431000\n",
      "1432000\n",
      "1433000\n",
      "1434000\n",
      "1435000\n",
      "1436000\n",
      "1437000\n",
      "1438000\n",
      "1439000\n",
      "1440000\n",
      "1441000\n",
      "1442000\n",
      "1443000\n",
      "1444000\n",
      "1445000\n",
      "1446000\n",
      "1447000\n",
      "1448000\n",
      "1449000\n",
      "1450000\n",
      "1451000\n",
      "1452000\n",
      "1453000\n",
      "1454000\n",
      "1455000\n",
      "1456000\n",
      "1457000\n",
      "1458000\n",
      "1459000\n",
      "1460000\n",
      "1461000\n",
      "1462000\n",
      "1463000\n",
      "1464000\n",
      "1465000\n",
      "1466000\n",
      "1467000\n",
      "1468000\n",
      "1469000\n",
      "1470000\n",
      "1471000\n",
      "1472000\n",
      "1473000\n",
      "1474000\n",
      "1475000\n",
      "1476000\n",
      "1477000\n",
      "1478000\n",
      "1479000\n",
      "1480000\n",
      "1481000\n",
      "1482000\n",
      "1483000\n",
      "1484000\n",
      "1485000\n",
      "1486000\n",
      "1487000\n",
      "1488000\n",
      "1489000\n",
      "1490000\n",
      "1491000\n",
      "1492000\n",
      "1493000\n",
      "1494000\n",
      "1495000\n",
      "1496000\n",
      "1497000\n",
      "1498000\n",
      "1499000\n",
      "1500000\n",
      "1501000\n",
      "1502000\n",
      "1503000\n",
      "1504000\n",
      "1505000\n",
      "1506000\n",
      "1507000\n",
      "1508000\n",
      "1509000\n",
      "1510000\n",
      "1511000\n",
      "1512000\n",
      "1513000\n",
      "1514000\n",
      "1515000\n",
      "1516000\n",
      "1517000\n",
      "1518000\n",
      "1519000\n",
      "1520000\n",
      "1521000\n",
      "1522000\n",
      "1523000\n",
      "1524000\n",
      "1525000\n",
      "1526000\n",
      "1527000\n",
      "1528000\n",
      "1529000\n",
      "1530000\n",
      "1531000\n",
      "1532000\n",
      "1533000\n",
      "1534000\n",
      "1535000\n",
      "1536000\n",
      "1537000\n",
      "1538000\n",
      "1539000\n",
      "1540000\n",
      "1541000\n",
      "1542000\n",
      "1543000\n",
      "1544000\n",
      "1545000\n",
      "1546000\n",
      "1547000\n",
      "1548000\n",
      "1549000\n",
      "1550000\n",
      "1551000\n",
      "1552000\n",
      "1553000\n",
      "1554000\n",
      "1555000\n",
      "1556000\n",
      "1557000\n",
      "1558000\n",
      "1559000\n",
      "1560000\n",
      "1561000\n",
      "1562000\n",
      "1563000\n",
      "1564000\n",
      "1565000\n",
      "1566000\n",
      "1567000\n",
      "1568000\n",
      "1569000\n",
      "1570000\n",
      "1571000\n",
      "1572000\n",
      "1573000\n",
      "1574000\n",
      "1575000\n",
      "1576000\n",
      "1577000\n",
      "1578000\n",
      "1579000\n",
      "1580000\n",
      "1581000\n",
      "1582000\n",
      "1583000\n",
      "1584000\n",
      "1585000\n",
      "1586000\n",
      "1587000\n",
      "1588000\n",
      "1589000\n",
      "1590000\n",
      "1591000\n",
      "1592000\n",
      "1593000\n",
      "1594000\n",
      "1595000\n",
      "1596000\n",
      "1597000\n",
      "1598000\n",
      "1599000\n",
      "1600000\n",
      "1601000\n",
      "1602000\n",
      "1603000\n",
      "1604000\n",
      "1605000\n",
      "1606000\n",
      "1607000\n",
      "1608000\n",
      "1609000\n",
      "1610000\n",
      "1611000\n",
      "1612000\n",
      "1613000\n",
      "1614000\n",
      "1615000\n",
      "1616000\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(test.shape[0])) :\n",
    "    test['InChI'][i] = text_preds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "smoking-minimum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000d2a601c</td>\n",
       "      <td>InChI=1S/C10H14BrN5S/c1-6-10(11)9(16(3)15-6)4-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f7fc849</td>\n",
       "      <td>InChI=1S/C15H18ClN3/c16-12-5-1-10(2-6-12)7-14-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000037687605</td>\n",
       "      <td>InChI=1S/C16H13BrN2O/c1-11(20)12-6-7-14(10-18)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00004b6d55b6</td>\n",
       "      <td>InChI=1S/C14H19FN4O/c1-14(2,3)12-13(16)17-18-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00004df0fe53</td>\n",
       "      <td>InChI=1S/C9H12O2/c10-3-1-2-4-5(6)7(11-9)3-8(4)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                              InChI\n",
       "0  00000d2a601c  InChI=1S/C10H14BrN5S/c1-6-10(11)9(16(3)15-6)4-...\n",
       "1  00001f7fc849  InChI=1S/C15H18ClN3/c16-12-5-1-10(2-6-12)7-14-...\n",
       "2  000037687605  InChI=1S/C16H13BrN2O/c1-11(20)12-6-7-14(10-18)...\n",
       "3  00004b6d55b6  InChI=1S/C14H19FN4O/c1-14(2,3)12-13(16)17-18-1...\n",
       "4  00004df0fe53  InChI=1S/C9H12O2/c10-3-1-2-4-5(6)7(11-9)3-8(4)..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aware-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('./submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-performer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
